<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Responsible AI in Lending — Simon Ju</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@500;700;900&family=Source+Serif+4:opsz@8..60&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
</head>
<body>
  <a href="#content" class="skip-link">Skip to main content</a>

  <div class="page">
    <div class="intro">
      <div class="intro-content"> 
        <h1 class="intro_name">Simon Ju</h1>
        <div class="profile">
          <img class="intro_avatar" src="pictures/profilepic.jpg" alt="Portrait"/>
          <div class="intro_bio">
            <p>Data Analyst turning data into insights across marketing performance and financial strategy.</p>
            <p>Python · SQL · Excel · Tableau · Power BI</p>
            <br>
          </div>
        </div>
        <nav class="intro_links">
          <ul>
            <li><a href="mailto:zj2113@nyu.edu">Email Me</a></li>
            <li>
            <a href="https://www.linkedin.com/in/simonju" target="_blank" rel="noopener noreferrer" aria-label="LinkedIn Profile" title="LinkedIn">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" class="linkedin-icon">
                <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/>
                </svg>
            </a>
            </li>
        </ul>
        </nav>
      </div> 
    </div>

    <main id="content" class="content">
      <a class="back" href="index.html#personal-projects">← Back</a>

      <header class="section" style="border-top:none; margin-top:0; padding-top:0;">
        <h2>Auditing a Loan Approval System for Fairness and Bias</h2>
        <p><strong>Tech:</strong> Python, scikit-learn, Responsible AI, Fairness Metrics, Machine Learning (Logistic Regression, Random Forest, XGBoost)</p>
        <img src="pictures/lending.jpg" alt="A preview of the project" class="project-image">
    </header>

      <section class="section">
        <h3>Project Summary</h3>
        <p>
          This project provides a comprehensive audit of an automated decision system (ADS) designed to predict loan application approvals. I implemented and evaluated three supervised learning models—**Logistic Regression, Random Forest, and XGBoost**—to assess their predictive accuracy. More importantly, the project focused on **Responsible Data Science**, analyzing the models for potential bias and fairness issues. Using fairness metrics, I examined whether the system produced disparate outcomes for different subgroups based on features like income, home ownership, and loan grade. The final analysis provides a clear recommendation on the model's deployment readiness and outlines steps for mitigating identified biases.
        </p>
      </section>

      <section class="section">
        <h3>The Business Problem</h3>
        <p>
          Automated loan approval systems are critical for efficiency in the financial industry, but they pose significant ethical risks. An algorithm trained on historical data can inherit and amplify existing societal biases, leading to discriminatory outcomes that unfairly deny access to credit for certain groups. This project addresses a core business and ethical question: **How can we build an accurate loan approval model that is also fair and equitable?** The goal was to audit a predictive system not just for its performance, but for its societal impact, ensuring that the drive for automation does not come at the cost of fairness.
        </p>
      </section>

      <section class="section">
        <h3>Key Code Snippets</h3>
        <p>The project involved building predictive models and then rigorously auditing them for fairness. The following snippets show these two key phases.</p>
        
        <h4>1. Training a Random Forest Classifier</h4>
        <p>A Random Forest model was trained to predict loan approval status. This model was chosen for its high accuracy and ability to handle complex interactions between features.</p>
        <div class="code-container">
          <pre><code><span class="token keyword">from</span> sklearn.ensemble <span class="token keyword">import</span> RandomForestClassifier
<span class="token keyword">from</span> sklearn.model_selection <span class="token keyword">import</span> train_test_split

<span class="token comment"># Define features (X) and target (y)</span>
features = [<span class="token string">'person_age'</span>, <span class="token string">'person_income'</span>, <span class="token string">'person_emp_length'</span>, <span class="token string">'loan_amnt'</span>, ...]
X = train_data[features]
y = train_data[<span class="token string">'loan_status'</span>]

<span class="token comment"># Split data and train the model</span>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="token number">0.2</span>, random_state=<span class="token number">42</span>)
rf_model = RandomForestClassifier(n_estimators=<span class="token number">100</span>, random_state=<span class="token number">42</span>)
rf_model.fit(X_train, y_train)</code></pre>
        </div>

        <h4>2. Auditing for Fairness Across Subgroups</h4>
        <p>After building the model, the most critical step was to evaluate its fairness. I wrote a function to calculate key performance metrics (Accuracy, Precision, Recall, F1 Score) for different subgroups based on sensitive attributes like home ownership.</p>
        <div class="code-container">
          <pre><code><span class="token keyword">from</span> sklearn.metrics <span class="token keyword">import</span> accuracy_score, precision_score, recall_score, f1_score

<span class="token keyword">def</span> <span class="token function">subgroup_analysis</span>(model, X_test, y_test, feature):
    results = {}
    <span class="token keyword">for</span> group <span class="token keyword">in</span> X_test[feature].unique():
        subgroup_indices = X_test[feature] == group
        y_subgroup_true = y_test[subgroup_indices]
        y_subgroup_pred = model.predict(X_test[subgroup_indices])
        
        results[group] = {
            <span class="token string">'Accuracy'</span>: accuracy_score(y_subgroup_true, y_subgroup_pred),
            <span class="token string">'Precision'</span>: precision_score(y_subgroup_true, y_subgroup_pred),
            <span class="token string">'Recall'</span>: recall_score(y_subgroup_true, y_subgroup_pred),
            <span class="token string">'F1 Score'</span>: f1_score(y_subgroup_true, y_subgroup_pred)
        }
    <span class="token keyword">return</span> pd.DataFrame(results)

<span class="token comment"># Run analysis on 'person_home_ownership'</span>
fairness_report = subgroup_analysis(rf_model, X_test, y_test, <span class="token string">'person_home_ownership'</span>)
<span class="token function">print</span>(fairness_report)</code></pre>
        </div>
      </section>

      <section class="section">
        <h3>Challenges</h3>
        <ul>
          <li><strong>Defining and Measuring Fairness:</strong> "Fairness" is not a single, universal metric. A major challenge was understanding the different types of fairness (e.g., demographic parity vs. equal opportunity) and selecting the appropriate metrics to evaluate the model's performance and potential for disparate impact.</li>
          <li><strong>The Accuracy-Fairness Trade-off:</strong> The analysis revealed that the most accurate models were not always the most fair. There was a clear trade-off, and the project required a nuanced discussion of how to balance the goal of high predictive performance with the ethical requirement to treat all subgroups equitably.</li>
          <li><strong>Proxy Variables and Hidden Bias:</strong> One of the biggest challenges in responsible AI is identifying "proxy variables"—features like loan grade or ZIP code that are not explicitly sensitive but are highly correlated with protected attributes. Auditing these proxies for hidden bias was a complex but essential part of the project.</li>
        </ul>
      </section>

      <a class="back" href="index.html#personal-projects">← Back</a>

      <footer class="site-footer">
        <p>© <span id="year">2025</span> Simon Ju</p>
      </footer>
    </main>
  </div>
</body>
</html>